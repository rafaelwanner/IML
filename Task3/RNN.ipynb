{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "herbal-baltimore",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "pediatric-lesbian",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 4\n",
    "AMMINO_LEN = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "superior-penetration",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv(\"task3_ks39mcp5/train.csv\")\n",
    "test_data = pd.read_csv(\"task3_ks39mcp5/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "persistent-drunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence(seq):\n",
    "    vector = [ord(letter) for letter in seq]\n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "floral-morris",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitmask(seq, alphabet=None):\n",
    "    if alphabet==None:\n",
    "        alphabet = ['R', 'H', 'K', 'D', 'E', 'S', 'T', 'N', 'Q', 'C', 'U', 'G', 'P', 'A', 'I', 'L', 'M', 'F', 'W', 'Y', 'V']\n",
    "    \n",
    "    vector = [[0 if char != letter else 1 for char in alphabet] for letter in seq]\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "entire-father",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = training_data['Active']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "approximate-float",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data):\n",
    "    X = np.zeros((data.shape[0], SEQ_LEN, AMMINO_LEN))\n",
    "    for i in range(data.shape[0]):\n",
    "        X[i,:,:] = bitmask(data.iloc[i, 0])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "extra-terrorist",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = process_data(training_data)\n",
    "X_test = process_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "orange-signature",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = Variable(torch.Tensor(X_train))\n",
    "X_test = Variable(torch.Tensor(X_test))\n",
    "\n",
    "y_train = Variable(torch.Tensor(y_train))\n",
    "y_train = torch.reshape(y_train, (y_train.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "frozen-isaac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([112000, 4, 21])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "perceived-density",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available, CPU used\n"
     ]
    }
   ],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ultimate-facial",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        #Defining the layers\n",
    "        # RNN Layer\n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)   \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        #Initializing hidden state for first input using method defined below\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "\n",
    "        # Passing in the input and hidden state into the model and obtaining outputs\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
    "         # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "affected-cigarette",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input_size=AMMINO_LEN, output_size=1, hidden_dim=21, n_layers=1)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "recognized-advice",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Params\n",
    "lr = 0.01\n",
    "n_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "boolean-reader",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "optimum-paradise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/100............. Loss: 0.3474\n",
      "Epoch: 20/100............. Loss: 0.1603\n",
      "Epoch: 30/100............. Loss: 0.1276\n",
      "Epoch: 40/100............. Loss: 0.1090\n",
      "Epoch: 50/100............. Loss: 0.0911\n",
      "Epoch: 60/100............. Loss: 0.0801\n",
      "Epoch: 70/100............. Loss: 0.0740\n",
      "Epoch: 80/100............. Loss: 0.0701\n",
      "Epoch: 90/100............. Loss: 0.0672\n",
      "Epoch: 100/100............. Loss: 0.0650\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.to(device)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    optimizer.zero_grad()\n",
    "    output, hidden = model(X_train)\n",
    "    output = output.to(device)\n",
    "    output = output[:,2:]\n",
    "    y_train = y_train.to(device)\n",
    "    loss = criterion(output, y_train.long())\n",
    "    loss.backward() # Does backpropagation and calculates gradients\n",
    "    optimizer.step() # Updates the weights accordingly\n",
    "    \n",
    "    if epoch%10 == 0:\n",
    "        print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
    "        print(\"Loss: {:.4f}\".format(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "noble-couple",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model(X_test)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "civic-blank",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = preds[:,3,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "intense-individual",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([48000])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "married-letter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.7529, -5.5822, -5.3998,  ..., -4.1693, -3.4740,  1.9178],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "preliminary-punch",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = np.zeros(len(preds))\n",
    "for i in range(len(preds)):\n",
    "    if preds[i]>0.5:\n",
    "        sub[i] = 1\n",
    "    else:\n",
    "        sub[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "geological-complement",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"submission.csv\", sub, fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affecting-roller",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
